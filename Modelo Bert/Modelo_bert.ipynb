{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":72539,"status":"ok","timestamp":1716336303289,"user":{"displayName":"David González","userId":"03736780622496018584"},"user_tz":-120},"id":"ES5wLQojNBn0","outputId":"2790f75d-6b2c-442d-8b65-cebecde1aa9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy>=0.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 1)) (1.25.2)\n","Requirement already satisfied: pandas>=0.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 2)) (2.0.3)\n","Requirement already satisfied: scikit-learn>=0.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 3)) (1.2.2)\n","Requirement already satisfied: matplotlib>=0.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 4)) (3.7.1)\n","Requirement already satisfied: transformers>=0.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 5)) (4.41.0)\n","Collecting accelerate>=0.0.0 (from -r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 6))\n","  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting optuna>=0.0.0 (from -r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 7))\n","  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting afinn>=0.0.0 (from -r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 8))\n","  Downloading afinn-0.1.tar.gz (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting emosent-py>=0.0.0 (from -r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 9))\n","  Downloading emosent_py-0.1.7-py3-none-any.whl (29 kB)\n","Collecting emoji>=0.0.0 (from -r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 10))\n","  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: xgboost>=0.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 11)) (2.0.3)\n","Requirement already satisfied: torch>=0.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12)) (2.3.0+cu121)\n","Requirement already satisfied: scipy>=0.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 14)) (1.11.4)\n","Requirement already satisfied: tqdm>=0.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 15)) (4.66.4)\n","Requirement already satisfied: joblib>=0.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 16)) (1.4.2)\n","Requirement already satisfied: dask>=0.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 17)) (2023.8.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 2)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 2)) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 2)) (2024.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 3)) (3.5.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 4)) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 4)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 4)) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 4)) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 4)) (24.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 4)) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 4)) (3.1.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 5)) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 5)) (0.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 5)) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 5)) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 5)) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 5)) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 5)) (0.4.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 6)) (5.9.5)\n","Collecting alembic>=1.5.0 (from optuna>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 7))\n","  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting colorlog (from optuna>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 7))\n","  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 7)) (2.0.30)\n","Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 10)) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12)) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12)) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12)) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12))\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12))\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12))\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12))\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12))\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12))\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12))\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12))\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12))\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12))\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12))\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12)) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12))\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 17)) (8.1.7)\n","Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 17)) (2.2.1)\n","Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 17)) (1.4.2)\n","Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 17)) (0.12.1)\n","Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 17)) (7.1.0)\n","Collecting Mako (from alembic>=1.5.0->optuna>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 7))\n","  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 17)) (3.18.2)\n","Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.2.0->dask>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 17)) (1.0.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 2)) (1.16.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 7)) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12)) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 5)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 5)) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 5)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 5)) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.0.0->-r /content/drive/MyDrive/basurilla/requirements_bert.txt (line 12)) (1.3.0)\n","Building wheels for collected packages: afinn\n","  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53430 sha256=19dff5eef46ebb298b2dd15564d53fae4b4da7bb152af68737cc62593fb1c046\n","  Stored in directory: /root/.cache/pip/wheels/b0/05/90/43f79196199a138fb486902fceca30a2d1b5228e6d2db8eb90\n","Successfully built afinn\n","Installing collected packages: emosent-py, afinn, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, Mako, emoji, colorlog, nvidia-cusparse-cu12, nvidia-cudnn-cu12, alembic, optuna, nvidia-cusolver-cu12, accelerate\n","Successfully installed Mako-1.3.5 accelerate-0.30.1 afinn-0.1 alembic-1.13.1 colorlog-6.8.2 emoji-2.12.1 emosent-py-0.1.7 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 optuna-3.6.1\n"]}],"source":["# Instalamos requerimientos\n","!pip install -r /content/drive/MyDrive/basurilla/requirements_bert.txt"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QnGPc_i4mt0w","executionInfo":{"status":"ok","timestamp":1716336151266,"user_tz":-120,"elapsed":24056,"user":{"displayName":"David González","userId":"03736780622496018584"}},"outputId":"fc1c980f-f067-41a1-940e-9da0350d94e1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"CQvklYz1mq4s"},"source":["IMPORTACION DE LIBRERIAS:|\n","--------------------------\n","\n","- data_preprocessing es un script donde se han agrupado las funciones especificas desarrolladas para el preprocesado de los datos originales que ha permitido agilizar el proceso iterativo de pruebas consecutivas:\n","    - load_data: Montar GDrive si es necesario + carga de csv + busqueda de duplicados, valores nulos y su eliminación si asi se decide\n","    - data_balance: balancear el numero de datos por clase de diagnóstico\n","    - min_tweets_filter: filtrar por numero de tweets\n","    - clean_tweets: Elimina de los tweets residuos del preprocesado y anonimización llevado a cabo por los investigadores del instituto tecnologico de monterrey.\n","    - group_tweets: agrupar tweets en grupos de n para que el modelo tenga un mayor contexto semantico\n","    - data_split_for_training: Split de datos para evaluacion de modelos\n","\n","- Ademas de las habituales para manejo de datos como Pandas y numpy, las metricas habituales y el split de datos, se han de importar también las bibliotecas especificas de pytorch, transformers de hugginface, optuna, clases predefinidas para el manejo de los datos, modulo para gestion de la memoria GPU, serialización de computos intermedios y gestores para el entrenamiento del modelo."]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":10757,"status":"ok","timestamp":1716336822704,"user":{"displayName":"David González","userId":"03736780622496018584"},"user_tz":-120},"id":"d0nkwxIvQXMS"},"outputs":[],"source":["# Manejo de datos\n","import pandas as pd\n","import numpy as np\n","\n","\n","# Funciones especificas de preprocesado de los datos\n","import preprocessing_functions2 as pf\n","\n","\n","import sklearn\n","# Split de datos para modelos de evaluación\n","from sklearn.model_selection import train_test_split\n","# Mtetricas de evaluacion\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n","\n","# PyTorch y manejo de GPU\n","import torch\n","\n","from torch.utils.data import (\n","    DataLoader,         # Clase que proporciona iteradores sobre un conjunto de datos.\n","    TensorDataset,      # Conjunto de datos simple que envuelve tensores.\n","    RandomSampler,      # Sampler que obtiene elementos de un conjunto de datos de manera aleatoria.\n","    SequentialSampler   # Sampler que obtiene elementos de un conjunto de datos de manera secuencial.\n",")\n","\n","import gc  # limpiar memoria GPU\n","\n","# Transformers de Hugging Face\n","import transformers\n","from transformers import (\n","    BertForSequenceClassification,  # Modelo preentrenado de BERT para clasificación de secuencias.\n","    BertModel,                      # Modelo base de BERT sin una cabeza de clasificación específica.\n","    BertTokenizer,                  # Tokenizador de texto específico de BERT.\n","    Trainer,                        # Módulo para gestionar y entrenar modelos de Transformers.\n","    TrainingArguments,              # Clase para definir los argumentos de entrenamiento.\n","    BertConfig,                     # Clase de configuración para los modelos BERT.\n","    EarlyStoppingCallback           # Callback para detener el entrenamiento temprano si no hay mejoras.\n",")\n","\n","\n","# Mostrar información periodica sobre la evolución del entrenamiento del modelo\n","transformers.logging.set_verbosity_info()\n","\n","# Optuna para optimización de hiperparámetros\n","import optuna\n","\n","# Optimización de entrenamiento\n","from accelerate import Accelerator\n","\n","# Serialización de modelos y datos\n","import pickle\n","\n","# Configura pandas para usar el modo copy-on-write\n","pd.options.mode.copy_on_write = True\n"]},{"cell_type":"markdown","metadata":{"id":"jNArT6cpmq4u"},"source":["Importar datos:|\n","---------------\n","#\n","- Funcion load_data\n","#\n","Balanceo de clases:|\n","---------------------\n","#\n","- Las mejores metricas se han encontrado balanceando los datos sobre el maximo de la segunda clase de diagnosticado mayoritaria \"DEPRESSION\".\n","\n","- Se ajusta a la baja la clase \"AHDH\" a 200000 muestras y el resto de clases permanecen igual.\n","#\n","Minimo de tweets:|\n","-----------------\n","#\n","- Se filtra el Dataframe para que los usuarios tengan un minimo de 40 tweets.\n","\n","- Este ajuste es recomendable teniendo en cuenta que los tweets se agruparan por usuario, de 20 en 20 o hasta completar el total de tweets de cada usuario\n","#"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TS2SXxHpdRXr"},"outputs":[],"source":["# Definimos la ruta donde se encuentra el CSV\n","file_path = '/content/drive/MyDrive/basurilla/df_concat_english.csv'\n","\n","# Llamamos a la funcion para crear el dataframe y gestionar duplicados y nulos\n","df = pf.load_data(file_path)\n","\n","# Llamado a la funcion para el balanceado de clases\n","df_balanced = pf.data_balance(df, 'class', 'CONTROL', 1000000, 225000)\n","\n","# Llamada a la funcion para filtrar por minimo de tweets por usuario\n","df_filtered = pf.min_tweets_filter(df_balanced, 40)\n","\n","# Llamada a la funcion para limpieza del texto de los tweets\n","df_clean = pf.clean_tweets(df_filtered, column='tweet')\n"]},{"cell_type":"markdown","metadata":{"id":"AT7nokQ3mq4u"},"source":["MAPEADO DE CLASES BINARIO|\n","-------------------------"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"yUHzNfATmq4u","executionInfo":{"status":"ok","timestamp":1716336971702,"user_tz":-120,"elapsed":1301,"user":{"displayName":"David González","userId":"03736780622496018584"}}},"outputs":[],"source":["# Se aplica una funcion para convertir la clase control y el resto de clases con usuarios diagnosticados en una salida binaria\n","df_clean['diagnosed'] = df_clean['class'].map(lambda x: 0 if x=='control' else 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rvtsFGJEmq4v"},"outputs":[],"source":["\n","# Llamada a la funcion para extraer un porcentaje de los datos para la busqueda de hiperparametros\n","df_training = pf.data_split_for_training(df_clean, 0.001, 'class')"]},{"cell_type":"markdown","metadata":{"id":"JGXkL6z5mq4v"},"source":["Definimos las columnas necesarias para el modelo final|\n","--------------------------------------------------------"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"AWercGpxta-J","executionInfo":{"status":"ok","timestamp":1716337370323,"user_tz":-120,"elapsed":397,"user":{"displayName":"David González","userId":"03736780622496018584"}}},"outputs":[],"source":["df_final = df_training[['tweet', 'diagnosed']]"]},{"cell_type":"markdown","metadata":{"id":"F-wtmh9Jmq4v"},"source":["Dataframe final|\n","---------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dtZZgsygylzv"},"outputs":[],"source":["# Ultima revision del dataframe final\n","print(df_final['diagnosed'].value_counts())\n","print(df_final.isna().sum())\n","print(df_final.duplicated().sum())"]},{"cell_type":"markdown","metadata":{"id":"rHH4V-zamq4w"},"source":["----------------------------------------------------------------------------------------------------------------------------------\n","----------------------------------------------------------------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"ABCynAO_dvFE"},"source":["TOKENIZACION|\n","------------\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FeEDqkJ5d_mx"},"outputs":[],"source":["# Dividir los datos en entrenamiento, validación y test\n","X_train, X_temp, y_train, y_temp = train_test_split(df_final, df_final['diagnosed'], test_size=0.3, random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","# Crear instancia de BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenizar los tweets\n","train_encodings = tokenizer(X_train['tweet'].tolist(), truncation=True, padding=True, return_tensors=\"pt\")\n","val_encodings = tokenizer(X_val['tweet'].tolist(), truncation=True, padding=True, return_tensors=\"pt\")\n","test_encodings = tokenizer(X_test['tweet'].tolist(), truncation=True, padding=True, return_tensors=\"pt\")\n"]},{"cell_type":"markdown","metadata":{"id":"k386m1Fnmq4w"},"source":["CREACION DE DATASETS|\n","--------------------"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"9cexolF8aweX","executionInfo":{"status":"ok","timestamp":1716337381094,"user_tz":-120,"elapsed":1,"user":{"displayName":"David González","userId":"03736780622496018584"}}},"outputs":[],"source":["# Definición de la Clase TweetDataset que hereda de torch.utils.data.Dataset para manejar los datos de tweets y sus etiquetas.\n","class TweetDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","    # __getitem__ toma los datos de los encodings y labels y devuelve un paquete con cada [tweet - etiqueta]\n","    def __getitem__(self, idx):\n","        # Capturar uno a uno los encodings de los tweets\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        # Etiquetas de salida\n","        item['labels'] = self.labels[idx]\n","        # Devolver esos datos empaquetados\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","#  Crear instancias de esta clase para los conjuntos de datos de entrenamiento, validación y prueba\n","train_dataset = TweetDataset(train_encodings, y_train.tolist())\n","val_dataset = TweetDataset(val_encodings, y_val.tolist())\n","test_dataset = TweetDataset(test_encodings, y_test.tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFRNl7Xfmq4w"},"outputs":[],"source":["# Exportar encodings de BERT\n","\n","# Encodings de los tweets\n","with open('/content/drive/MyDrive/Colab Notebooks/train_encodings.pkl', 'wb') as f:\n","    pickle.dump(train_encodings, f)\n","with open('/content/drive/MyDrive/Colab Notebooks/val_encodings.pkl', 'wb') as f:\n","    pickle.dump(val_encodings, f)\n","with open('/content/drive/MyDrive/Colab Notebooks/test_encodings.pkl', 'wb') as f:\n","    pickle.dump(test_encodings, f)\n","\n","# Etiquetas verdaderas\n","with open('/content/drive/MyDrive/Colab Notebooks/y_train.pkl', 'wb') as f:\n","    pickle.dump(y_train, f)\n","with open('/content/drive/MyDrive/Colab Notebooks/y_val.pkl', 'wb') as f:\n","    pickle.dump(y_val, f)\n","with open('/content/drive/MyDrive/Colab Notebooks/y_test.pkl', 'wb') as f:\n","    pickle.dump(y_test, f)\n","\n","\n","# Datasets finales\n","with open('/content/drive/MyDrive/Colab Notebooks/train_dataset.pkl', 'wb') as f:\n","    pickle.dump(train_dataset, f)\n","with open('/content/drive/MyDrive/Colab Notebooks/val_dataset.pkl', 'wb') as f:\n","    pickle.dump(val_dataset, f)\n","with open('/content/drive/MyDrive/Colab Notebooks/test_dataset_.pkl', 'wb') as f:\n","    pickle.dump(test_dataset, f)"]},{"cell_type":"markdown","metadata":{"id":"0u0rjZfDmq4w"},"source":["GESTION DEL DISPOSITIVO DE COMPUTO|\n","----------------------------------"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":955,"status":"ok","timestamp":1716337230057,"user":{"displayName":"David González","userId":"03736780622496018584"},"user_tz":-120},"id":"_rH5jIAXb0O-","outputId":"1843cb8d-fc65-4a95-b637-ec634c5908b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA no disponible. Entrenando en CPU.\n"]}],"source":["# Definicion del dispositivo a usar segun disponibilidad (preferentemente CUDA)\n","\n","import torch\n","\n","if torch.cuda.is_available():\n","    print(\"CUDA disponible. Entrenando en GPU.\")\n","    device = torch.device(\"cuda\")\n","else:\n","    print(\"CUDA no disponible. Entrenando en CPU.\")\n","    device = torch.device(\"cpu\")\n"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"EkNiZ0LTmq4w","executionInfo":{"status":"ok","timestamp":1716337232082,"user_tz":-120,"elapsed":1,"user":{"displayName":"David González","userId":"03736780622496018584"}}},"outputs":[],"source":["# limpiar memoria gpu\n","\n","gc.collect()           # Recolecta basura en el nivel de Python\n","torch.cuda.empty_cache()  # Limpia la caché de CUDA"]},{"cell_type":"markdown","metadata":{"id":"k7_qmagbmq4w"},"source":["MODELO: DEFINIR, EJECUTAR Y MOSTRAR OPTIMIZACIÓN |\n","---------------------------------------------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZTqIRJJ-a8Ka"},"outputs":[],"source":["# Definimnos función para calcular métricas durante la evaluación\n","def compute_metrics(pred):\n","    print(\"Computing metrics...\")  # Confirma que la función ha sido llamada\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n","    acc = accuracy_score(labels, preds)\n","    auc = roc_auc_score(labels, pred.predictions[:, 1])\n","    print(f\"Computed accuracy: {acc}\")  # mostrar resultado de accuracy\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall,\n","        'auc': auc\n","    }\n","\n","# Definicion de la función objetivo de optuna\n","def objective(trial):\n","    # Instanciamos Bertconfig para poder definir algunos parametros propios del modelo\n","    config = BertConfig.from_pretrained('bert-base-uncased')\n","    config.num_labels = 2\n","\n","    # Espacio de hiperparametros para optimizar\n","    config.hidden_dropout_prob = trial.suggest_float(\"hidden_dropout_prob\", 0.1, 0.5)                   # Probabilidad de droptout en las capas ocultas\n","    config.attention_probs_dropout_prob = trial.suggest_float(\"attention_probs_dropout_prob\", 0.1, 0.5) # Probabilidad de droptout del mecanismo de atención\n","    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-4, log=True)\n","    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n","    weight_decay = trial.suggest_float('weight_decay', 0.0, 0.1)\n","    warmup_steps = trial.suggest_int('warmup_steps', 0, 1000)\n","    eval_steps = trial.suggest_int('eval_steps', 500, 5000)\n","\n","    # Creamos instancia del modelo\n","    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n","    # Enviamos modelo a la GPU si esta disponible\n","    model = model.to(device)\n","\n","\n","    # Argumentos para el trainer\n","    output_dir = f'./results/trial_{trial.number}'\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,                       # directorio de salida para los resultados del modelo\n","        num_train_epochs=3,                          # número total de épocas de entrenamiento\n","        evaluation_strategy='steps',                 # estrategia de evaluación periodica\n","        eval_steps = eval_steps,                     # pasos completados hasta la evaluación periodica\n","        save_steps= eval_steps,                      # pasos completados hasta el guardado de un checkpoint\n","        per_device_train_batch_size=batch_size,      # tamaño del batch para entrenamiento\n","        per_device_eval_batch_size=batch_size,       # tamaño del batch para evaluación\n","        warmup_steps=warmup_steps,                   # pasos hasta el incremento maximo de learning rate al inicio\n","        weight_decay=weight_decay,                   # fuerza de la decay del peso\n","        logging_dir=f'./logs/trial_{trial.number}',  # directorio para almacenar logs\n","        load_best_model_at_end=True,                 # guardar un checkpoint al final de cada época\n","        logging_steps=10,                            # número máximo de checkpoints a guardar\n","        save_strategy='steps',                       # estrategia de checkpoint\n","        report_to='none'\n","    )\n","\n","\n","\n","\n","    # Inicializar el Trainer con los argumentos de entrenamiento\n","    early_stopping = EarlyStoppingCallback(early_stopping_patience=7, early_stopping_threshold=0.01)\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset,\n","        compute_metrics=compute_metrics,\n","        callbacks=[early_stopping]\n","    )\n","\n","    trainer.train()\n","    eval_results = trainer.evaluate()\n","    print(eval_results)\n","\n","    loss = eval_results.get(\"eval_loss\")\n","    if loss is None:\n","        raise ValueError(\"Accuracy could not be computed.\")\n","\n","    return loss\n","\n","\n","# Creación del estudio de optuna\n","study = optuna.create_study(direction='minimize')\n","# Llamada a la instancia recien creada de optuna definiendo funcion objetivo y numero de pruebas\n","study.optimize(objective, n_trials=10)\n","\n","\n","# Una vez completado el estudio, obtener el mejor ensayo\n","best_trial = study.best_trial\n","\n","# Muestra los mejores parámetros\n","print(\"Mejores parámetros: \", best_trial.params)\n","\n","# Mejor valor de la función objetivo\n","print(\"Mejor valor (pérdida mínima):\", best_trial.value)\n"]},{"cell_type":"markdown","metadata":{"id":"CA8Ry3QUmq4x"},"source":["ENTRENAMIENTO FINAL CON LOS MEJORES HIPERPARAMETROS ENCONTRADOS|\n","------------------------------------------------------------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3hchy0I_MLs"},"outputs":[],"source":["#Configuracion del modelo\n","config = BertConfig.from_pretrained('bert-base-uncased')\n","config.hidden_dropout_prob = best_trial.params['hidden_dropout_prob']\n","config.attention_probs_dropout_prob = best_trial.params['attention_probs_dropout_prob']\n","config.num_labels = 2\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n","model = model.to(device)\n","\n","# Funcion de métricas\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n","    acc = accuracy_score(labels, preds)\n","    auc = roc_auc_score(labels, pred.predictions[:, 1])\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall,\n","        'auc': auc\n","    }\n","\n","output_dir = '/content/drive/MyDrive/Colab Notebooks/proyecto final bootcamp/berts/bert_model_balanced_final/training_checkpoints'\n","training_args = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=3,\n","    evaluation_strategy='steps',\n","    learning_rate=best_trial.params['learning_rate'],\n","    eval_steps=2000,\n","    save_steps=2000,\n","    per_device_train_batch_size=best_trial.params['batch_size'],\n","    per_device_eval_batch_size=best_trial.params['batch_size'],\n","    warmup_steps=best_trial.params['warmup_steps'],\n","    weight_decay=best_trial.params['weight_decay'],\n","    load_best_model_at_end=True,\n","    save_strategy='steps'\n",")\n","\n","early_stopping = EarlyStoppingCallback(early_stopping_patience=7, early_stopping_threshold=0.01)\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics,\n","    callbacks=[early_stopping]\n",")\n","\n","# Inicializar entrenamiento\n","trainer.train()\n"]},{"cell_type":"markdown","metadata":{"id":"od8q2xcSmq4x"},"source":["EVALUACIÓN DE LOS RESULTADOS DEL ENTRENAMIENTO|\n","--------------------------------------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_k4eoyImq4x"},"outputs":[],"source":["\n","# Evaluar el modelo en el conjunto de entrenamiento\n","train_metrics = trainer.evaluate(train_dataset)\n","print(\"Train Metrics:\", train_metrics)\n","\n","# Evaluar el modelo en el conjunto de validación\n","val_metrics = trainer.evaluate(val_dataset)\n","print(\"Validation Metrics:\", val_metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ap3qQb3dT8bX"},"outputs":[],"source":["# Obtener las predicciones en el conjunto de datos de prueba\n","predictions = trainer.predict(test_dataset)\n","\n","# Calcular las métricas utilizando las predicciones y las etiquetas verdaderas\n","metrics = compute_metrics(predictions)\n","\n","# Imprimir las métricas\n","print(\"Accuracy:\", metrics['accuracy'])\n","print(\"Precision:\", metrics['precision'])\n","print(\"Recall:\", metrics['recall'])\n","print(\"F1 Score:\", metrics['f1'])\n","print(\"AUC:\", metrics['auc'])"]},{"cell_type":"markdown","metadata":{"id":"2wEV7wEmmq4x"},"source":["---------------------------------------------------------------------------------------------------------------------------------------\n","---------------------------------------------------------------------------------------------------------------------------------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h78BLrTlmq4x"},"outputs":[],"source":["# Guardar el modelo final\n","model.save_pretrained('/content/drive/MyDrive/Colab Notebooks')\n"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1OuTxAdUgbBe70wv3zEBOR9E5PR7pA0XU","timestamp":1715667617922},{"file_id":"13G9xi--LtTa_TTvNSY7blkKPkceiUmE_","timestamp":1715201450450},{"file_id":"1eDZ6GT8kT_FtO4elmCYFRINB29jPTLYt","timestamp":1714220561404},{"file_id":"1F1u5o1fWwlRDxQzN55pMNAoeQKsIs7jF","timestamp":1714167953914}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":0}